{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement learning with Policy gradient (REINFORCE)\n\nFor an introduction to policy gradient methods, see [Levine 2017 (slides)](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_4_policy_gradient.pdf)\n\nThis tutorial is available as an [iJulia notebook](https://github.com/baggepinnen/baggepinnen.github.io/blob/master/reinforce.ipynb)\n\nWe start out by importing some packages. If it's your first time, you might have to install some packages using the commands in the initial comment.\nOpenAI gym is installed with instructions available at https://github.com/JuliaML/OpenAIGym.jl"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# using Pkg\n# Pkg.add(\"Plots\")\n# Pkg.add(\"BasisFunctionExpansions\")\n# Pkg.add(\"ValueHistories\")\n# Pkg.add(\"https://github.com/JuliaML/OpenAIGym.jl\")\n\nusing OpenAIGym, BasisFunctionExpansions, ValueHistories, Plots, Random, LinearAlgebra"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to perform plotting in the loop, the following function helps keeping the plot clean."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "default(size=(1200,800)) # Set the default plot size\nfunction update_plot!(p; max_history = 10)\n    num_series = length(p.series_list)\n    if num_series > 1\n        if num_series > max_history\n            deleteat!(p.series_list,1:num_series-max_history)\n        end\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define an environment from the OpenAIGym framework, we'll use the [cartpole environment](https://gym.openai.com/envs/CartPole-v0/) and a function that runs an entire episode."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "env = GymEnv(\"CartPole-v0\")\n\nfunction collect_episode(ep)\n    s,a,r,s1 = Vector{Vector{Float64}}(),Vector{Float64}(),Vector{Float64}(),Vector{Vector{Float64}}()\n    for (ss,aa,rr,ss1) in ep\n        push!(s,ss)\n        push!(a,aa)\n        push!(r,rr)\n        push!(s1,ss1)\n    end\n    s,a,r,s1\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also define a policy object that is a linear combination of radial basis functions. For this we make use of the package [`BasisFunctionExpansions.jl`](https://github.com/baggepinnen/BasisFunctionExpansions.jl). We use 4 basis functions along each dimension. Along with the policy, we define a function to calculate $\\nabla_\\theta \\log \\pi_\\theta(a|s)$"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "bfe = MultiUniformRBFE([LinRange(-0.3,0.3,3) LinRange(-2,2,3) LinRange(-0.2,0.2,3) LinRange(-3.2,3.2,3)], [4,4,4,4])\n\nmutable struct Policy <: AbstractPolicy\n    θ::Vector{Float64}\n    bfe::MultiUniformRBFE\n    σ::Float64\nend\n\n(π::Policy)(s) = π.bfe(s)⋅π.θ + 0.5 # Mean of policy function\n\nfunction Reinforce.action(π::Policy, r, s, A)\n    π(s) + π.σ*randn() > 0.5 ? 1 : 0\nend\n\nfunction ∇logπ(s,a)\n    0.5/π.σ^2*(a-π(s))*π.bfe(s)\nend\n\nnum_epochs       = 100\nN                = 5\nα                = 0.00001 # Initial learning rate\nconst decay_rate = 0.99  # decay rate for learning rate and noise\nconst γ          = 0.99  # Discounting factor\nσ                = 0.1   # Policy noise\nconst π = Policy(0.001randn(size(bfe.μ,2)), bfe, σ); # π is now our policy object"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now arrived at the main algorithm. We wrap it in a function for the Julia JIT complier to have it run faster."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "gr() # Set GR as plot backend, try pyplot() if GR is not working\nfunction REINFORCE(π, num_epochs, N, α; plotting=true)\n    π.θ .= 0.001randn(size(bfe.μ,2)) # Reset policy parameters\n    plotting && (fig = plot(layout=2, show=true))\n    reward_history = ValueHistories.History(Float64)\n    for i = 1:num_epochs\n        local ep\n        α   *= decay_rate # Decay the learning rate\n        π.σ *= decay_rate # Decay the exploration noise\n        ∇θ   = zeros(size(π.θ)) # Gradient accumulator\n        for n = 1:N # Collect N trajectories for Expectation estimation\n            ep       = Episode(env, π)\n            s,a,r,s1 = collect_episode(ep)\n            sumr     = 0.\n            for t = length(s):-1:1 # Iterate backwards for efficient ∑r calculation\n                sumr += r[t] # Sum of rewards\n                ∇θ .+= ∇logπ(s[t],a[t]) * sumr\n            end\n            push!(reward_history, (i-1)*N+n, ep.total_reward)\n        end\n        π.θ .+= α/N * ∇θ # Take a gradient step on policy parameters\n\n        # Printing and plotting\n        i % 5 == 0 && println(\"Epoch: $i, reward: $(ep.total_reward)\")\n        if plotting && i % 5 == 0\n            plot!(reward_history, subplot=1)\n            scatter!(π.θ, subplot=2, c=:red, title=\"Policy parameters\")\n            update_plot!(fig[1], max_history=1)\n            update_plot!(fig[2], max_history=5)\n            gui(fig)\n        end\n    end\n    plot(reward_history, title=\"Rewards\", xlabel=\"Episodes\", show=true)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now call our function."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Random.seed!(1);\nREINFORCE(π, num_epochs, N, α; plotting=false)"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.1.0"
    },
    "kernelspec": {
      "name": "julia-1.1",
      "display_name": "Julia 1.1.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
