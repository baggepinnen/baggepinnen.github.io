{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement learning with Cross-entropy method optimization (CEM)\n\nCEM performs a policy search with exploration directly in parameter space. A collection of sample policies are sampled from a distribution and tried through experiments. A new, weighted distribution is fit to the sampled policy parameters, where the reward achieved by the policy determines its weight. This way, policies that achieved high rewards are more likely to be sampled next time. This procedure is iterated until performance is adequate.\n\nWe focus on Gaussian distributions in parameter space, and illustrate two methods of fitting\nnew weighted distributions: rank-based and Boltzmann-based. The rank-based fit fits and unweighted Gaussian to the top fraction of policies, whereas the Boltzmann-based strategy assigns a weights of $\\exp(r_i)$ to policy $i$, where $r_i$ is the reward achieved by that policy.\n\nFor further introduction, see [Levine 2017 (slides)](http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_7_advanced_q_learning.pdf)\n\nThis tutorial is available as an [iJulia notebook](https://github.com/baggepinnen/baggepinnen.github.io/blob/master/cem.ipynb)\n\nWe start out by importing some packages. If it's your first time, you might have to install some packages using the commands in the initial comment.\nOpenAI gym is installed with instructions available at https://gym.openai.com/docs/"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# Pkg.clone(\"https://github.com/tbreloff/MetaPkg.jl\")\n# using MetaPkg\n# meta_add(\"MetaRL\")\n# Pkg.add(\"Plots\")\n# Pkg.add(\"BasisFunctionExpansions\")\n# Pkg.add(\"ValueHistories\")\n\nusing OpenAIGym, ValueHistories, Plots"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to perform plotting in the loop, the following function helps keeping the plot clean."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "default(size=(1200,800)) # Set the default plot size\nfunction update_plot!(p; max_history = 10, attribute = :markercolor)\n    num_series = length(p.series_list)\n    if num_series > 1\n        if num_series > max_history\n            deleteat!(p.series_list,1:num_series-max_history)\n        end\n        for i = 1:min(max_history, num_series)-1\n            alpha = 1-2/max_history\n            c = p[i][attribute]\n            b = alpha*c.b + (1-alpha)*0.5\n            g = alpha*c.g + (1-alpha)*0.5\n            r = alpha*c.r + (1-alpha)*0.5\n            a = alpha*c.alpha\n            p[i][attribute] = RGBA(r,g,b,a)\n        end\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define an environment from the OpenAIGym framework, we'll use the [cartpole environment](https://gym.openai.com/envs/CartPole-v0/) and a function that runs an entire episode."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "env = GymEnv(\"CartPole-v0\")\n\nfunction collect_reward(ep)\n    r = Vector{Float64}(0)\n    for (ss,aa,rr,ss1) in ep\n        push!(r,rr)\n    end\n    r\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also define a linear policy object that is a linear combination of the states."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "struct Policy <: AbstractPolicy\n    θ::Vector{Float64}\nend\n\n(π::Policy)(s) = π.θ's + 0.5 > 0.5 ? 1 :  0 # policy function\n\nReinforce.action(π::Policy, r, s, A) = π(s)\n\nnum_params  = length(env.state)\nnum_epochs  = 15\nnum_samples = 25\nΣ           = 0.2eye(num_params) # Initial noise chol(covariance)\nμ           = zeros(num_params) # Initial policy mean\nconst π     = Policy(zeros(num_params)) # π is now our policy object"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now arrived at the main algorithm. We wrap it in a function for the Julia JIT complier to have it run faster."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function fit_new_distribution_weighted(θs, w)\n    weights = ProbabilityWeights(exp.(w))\n    μ       = mean(θs, weights, 2)\n    Σ       = cov(θs, weights, 2, corrected=true)\n    Σ       = chol(Symmetric(Σ + 1e-5I))'\n    μ, Σ\nend\n\nfunction fit_new_distribution_rank(θs, w)\n    fraction = 2\n    p        = sortperm(w)\n    indicies = p[end-end÷fraction:end]\n    μ        = mean(θs[:,indicies], 2)\n    Σ        = cov(θs[:,indicies], 2)\n    Σ        = chol(Symmetric(Σ))'\n    μ, Σ\nend\n\n\ngr() # Set GR as plot backend, try pyplot() if GR is not working\nfunction CME(π, μ, Σ, num_epochs, num_samples; plotting=true)\n    plotting && (fig = plot(layout=2, show=true))\n    reward_history = ValueHistories.History(Float64)\n    for i = 1:num_epochs\n        θs = Σ*randn(num_params, num_samples) .+ μ # Sample new policies\n        weights = zeros(num_samples)\n        for n = 1:num_samples # Collect N trajectories for Expectation estimation\n            π.θ       .= θs[:,n] # Set policy parameters\n            ep         = Episode(env, π)\n            s,a,r,s1   = collect_reward(ep)\n            weights[n] = ep.total_reward\n            push!(reward_history, (i-1)*num_samples+n, ep.total_reward)\n        end\n        μ, Σ = fit_new_distribution(θs, weights)\n\n        # Printing and plotting\n        println(\"Epoch: $i, reward: $(reward_history.values[end])\")\n        if plotting\n            plot!(reward_history, subplot=1, show=false)\n            scatter!(θs, subplot=2, c=:red, title=\"Policy parameters\", show=false, legend=false)\n            update_plot!(fig[1], max_history=1, attribute=:linecolor)\n            update_plot!(fig[2], max_history=num_samples, attribute=:linecolor)\n            gui(fig)\n        end\n    end\n    plot(reward_history, title=\"Rewards\", xlabel=\"Episodes\", show=true)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now call our function, with the two different distribution-refit strategies:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fit_new_distribution = fit_new_distribution_weighted\nsrand(1);\nCME(π, μ, Σ, num_epochs, num_samples; plotting=true)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "fit_new_distribution = fit_new_distribution_rank\nsrand(1);\nCME(π, μ, Σ, num_epochs, num_samples; plotting=true)"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "0.5.0"
    },
    "kernelspec": {
      "name": "julia-0.5",
      "display_name": "Julia 0.5.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
