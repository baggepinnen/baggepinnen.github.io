{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement learning algorithms\n\n## Q-learning with ϵ-greedy\nThis tutorial is available as an [iJulia notebook](https://github.com/baggepinnen/baggepinnen.github.io/blob/master/ql.ipynb)\n\nWe start out by importing some packages. If it's your first time, you might have to install some packages using the commands in the initial comment.\nOpenAI gym is installed with instructions available at https://github.com/JuliaML/OpenAIGym.jl"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# using Pkg\n# Pkg.add(\"Plots\")\n# Pkg.add(\"BasisFunctionExpansions\")\n# Pkg.add(\"ValueHistories\")\n# Pkg.add(\"https://github.com/JuliaML/OpenAIGym.jl\")\n\nusing OpenAIGym, BasisFunctionExpansions, ValueHistories, Plots, Random, LinearAlgebra"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to perform plotting in the loop, the following function helps keeping the plot clean."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "default(size=(1200,800)) # Set the default plot size\nfunction update_plot!(p; max_history = 10, attribute = :markercolor)\n    num_series = length(p.series_list)\n    if num_series > 1\n        if num_series > max_history\n            deleteat!(p.series_list,1:num_series-max_history)\n        end\n        for i = 1:min(max_history, num_series)-1\n            alpha = 1-2/max_history\n            c = p[i][attribute]\n            b = alpha*c.b + (1-alpha)*0.5\n            g = alpha*c.g + (1-alpha)*0.5\n            r = alpha*c.r + (1-alpha)*0.5\n            a = alpha*c.alpha\n            p[i][attribute] = RGBA(r,g,b,a)\n        end\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define an environment from the OpenAIGym framework, we'll use the [cartpole environment](https://gym.openai.com/envs/CartPole-v0/)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "env = GymEnv(\"CartPole-v0\");"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also define a $Q$-function approximator that is a linear combination of radial basis functions. For this we make use of the package [`BasisFunctionExpansions.jl`](https://github.com/baggepinnen/BasisFunctionExpansions.jl). We use 4 basis functions along each dimension except for the action dimension, which is discrete with two values only."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "bfe = MultiUniformRBFE([LinRange(-0.3,0.3,3) LinRange(-2,2,3) LinRange(-0.2,0.2,3) LinRange(-3.2,3.2,3) LinRange(0,1,3)], [4,4,4,4,2])\n\nstruct Qfun\n    θ::Vector{Float64}\n    bfe::MultiUniformRBFE\nend\n\n(Q::Qfun)(s,a) = Q.bfe([s;a])⋅Q.θ # This row makes our type Qfun callable\n\n\"\"\"This function makes for a nice syntax of updating the Q-function\"\"\"\nfunction Base.setindex!(Q::Qfun, q, s, a)\n    Q.θ .+= Q.bfe([s;a])* q\nend\n\nconst Q = Qfun(zeros(size(bfe.μ,2)), bfe); # Q is now our Q-function approximator"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We move on by defining some parameters"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "num_episodes     = 400\nα                = 1.    # Initial learning rate\nconst ϵ          = 0.5   # Initial chance of choosing random action\nconst decay_rate = 0.995 # decay rate for learning rate and ϵ\nconst γ          = 0.99; # Discounting factor"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to define a `Reinforce.jl` policy type"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "mutable struct ϵGreedyPolicy <: AbstractPolicy\n    ϵ::Float64\n    decay_rate::Float64\nend\n\n\"\"\"Calling this function decays the ϵ\"\"\"\nfunction decay!(policy::ϵGreedyPolicy)\n    policy.ϵ *= policy.decay_rate\nend\n\n\"\"\"This is our ϵ-greedy action function\"\"\"\nfunction Reinforce.action(policy::ϵGreedyPolicy, r, s, A)\n    rand() < policy.ϵ ? rand(0:1) : Q(s,1) > Q(s,0) ? 1 : 0\nend\n\n\"\"\"max(Q(s,a)) over a\"\"\"\nfunction max_a(Q, s)\n    max(Q(s,1), Q(s,0))\nend\n\npolicy = ϵGreedyPolicy(ϵ, decay_rate);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now arrived at the main algorithm. We wrap it in a function for the Julia JIT complier to have it run faster."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function Qlearning(policy, num_episodes, α; plotting=true)\n    plotting && (fig = plot(layout=2, show=true))\n    reward_history = ValueHistories.History(Float64)\n    for i = 1:num_episodes\n        ep = Episode(env, policy)\n        α *= decay_rate # Decay the learning rate\n        decay!(policy) # Decay greedyness\n        for (s,a,r,s1) in ep # An episode object is iterable\n            # i % 100 == 0 && OpenAIGym.render(env) # Uncomment this line to have the episode rendered\n            Q[s,a] = α*(r + γ*max_a(Q, s1) - Q(s,a)) # Update the Q-function approximator using Q-learning\n        end\n        push!(reward_history, i, ep.total_reward)\n        i % 20 == 0 && println(\"Episode: $i, reward: $(ep.total_reward)\")\n        if plotting && i % 20 == 0\n            plot!(reward_history, subplot=1)\n            scatter!(Q.θ, subplot=2, c=:red, title=\"Policy parameters\")\n            update_plot!(fig[1], max_history=1, attribute=:linecolor)\n            update_plot!(fig[2], max_history=5, attribute=:linecolor)\n            gui(fig)\n        end\n    end\n    plot(reward_history, title=\"Rewards\", xlabel=\"Episode\", show=true)\nend;"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now call our function."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Random.seed!(0);\n@time Qlearning(policy, num_episodes, α, plotting = false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boltzmann exploration\nThe ϵ-greedy works well and learns to achieve maximum reward after a few hundred iterations.\nA different policy, that works well for discrete action spaces is Boltzmann exploration.\nConsider the policy $π(a|s) = \\exp(Q(s,a))/\\sum_a \\exp(Q(s,a))$, it will sample an action based on how good Q-value it has. This ensures that all actions that have some possibility to be good are sampled, but actions known to be very good are sampled more often."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "struct BoltzmannPolicy <: AbstractPolicy end\n\ndecay!(policy) = nothing\n\n\"\"\"This is our Boltzmann exploration action function\"\"\"\nfunction Reinforce.action(policy::BoltzmannPolicy, r, s, A)\n    Q1,Q0 = Q(s,1), Q(s,0)\n    prob1 = exp(Q1)/(exp(Q1)+exp(Q0))\n    rand() < prob1 ? 1 : 0\nend\n\npolicy = BoltzmannPolicy()\nQ.θ  .*= 0; # Reset Q-function"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "Random.seed!(0);\n@time Qlearning(policy, num_episodes, α, plotting = false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Boltzmann exploration typically works better than ϵ-greedy in this environment."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.1.0"
    },
    "kernelspec": {
      "name": "julia-1.1",
      "display_name": "Julia 1.1.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
